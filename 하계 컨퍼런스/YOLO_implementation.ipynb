{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## YOLOv1 구현"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import xmltodict\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# gpu 사용량 제한\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
    "    print(e)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.allow_growth = True \n",
    "session = tf.compat.v1.Session(config=config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 데이터 전처리"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 파일 경로\n",
    "train_x_path = '/home/ubuntu/CUAI_2021/Advanced_Minkyu_Kim/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/JPEGImages'\n",
    "train_y_path = '/home/ubuntu/CUAI_2021/Advanced_Minkyu_Kim/PASCAL_VOC_2007/train/VOCdevkit/VOC2007/Annotations'\n",
    "\n",
    "test_x_path = '/home/ubuntu/CUAI_2021/Advanced_Minkyu_Kim/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/JPEGImages'\n",
    "test_y_path = '/home/ubuntu/CUAI_2021/Advanced_Minkyu_Kim/PASCAL_VOC_2007/test/VOCdevkit/VOC2007/Annotations'\n",
    "\n",
    "# 파일 경로 휙득\n",
    "image_file_path_list = sorted([x for x in glob(train_x_path + '/**')])\n",
    "xml_file_path_list = sorted([x for x in glob(train_y_path + '/**')])\n",
    "\n",
    "test_image_file_path_list = sorted([x for x in glob(test_x_path + '/**')])\n",
    "test_xml_file_path_list = sorted([x for x in glob(test_y_path + '/**')])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 데이터셋에 존재하는 클래스가 얼마나 있는지 알아낸다\n",
    "def get_Classes_inImage(xml_file_list):\n",
    "    Classes_inDataSet = []\n",
    "\n",
    "    for xml_file_path in xml_file_list: \n",
    "\n",
    "        f = open(xml_file_path)\n",
    "        xml_file = xmltodict.parse(f.read())\n",
    "        # 사진에 객체가 여러개 있을 경우\n",
    "        try: \n",
    "            for obj in xml_file['annotation']['object']:\n",
    "                Classes_inDataSet.append(obj['name'].lower()) # 들어있는 객체 종류를 알아낸다\n",
    "        # 사진에 객체가 하나만 있을 경우\n",
    "        except TypeError as e: \n",
    "            Classes_inDataSet.append(xml_file['annotation']['object']['name'].lower()) \n",
    "        f.close()\n",
    "\n",
    "    Classes_inDataSet = list(set(Classes_inDataSet)) # set은 중복된걸 다 제거하고 유니크한? 아무튼 하나만 가져온다. 그걸 리스트로 만든다\n",
    "    Classes_inDataSet.sort() # 정렬\n",
    "\n",
    "    return Classes_inDataSet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# 이미지에 어떤 Ground Truth Box가 있는지(label 휙득)\n",
    "def get_label_fromImage(xml_file_path, Classes_inDataSet):\n",
    "\n",
    "    f = open(xml_file_path)\n",
    "    xml_file = xmltodict.parse(f.read()) \n",
    "\n",
    "    Image_Height = float(xml_file['annotation']['size']['height'])\n",
    "    Image_Width  = float(xml_file['annotation']['size']['width'])\n",
    "\n",
    "    label = np.zeros((7, 7, 25), dtype = float)\n",
    "    \n",
    "    try:\n",
    "        for obj in xml_file['annotation']['object']:\n",
    "            \n",
    "            # class의 index 휙득\n",
    "            class_index = Classes_inDataSet.index(obj['name'].lower())\n",
    "            \n",
    "            # min, max좌표 얻기\n",
    "            x_min = float(obj['bndbox']['xmin']) \n",
    "            y_min = float(obj['bndbox']['ymin'])\n",
    "            x_max = float(obj['bndbox']['xmax']) \n",
    "            y_max = float(obj['bndbox']['ymax'])\n",
    "\n",
    "            # 224*224에 맞게 변형시켜줌\n",
    "            x_min = float((224.0/Image_Width)*x_min)\n",
    "            y_min = float((224.0/Image_Height)*y_min)\n",
    "            x_max = float((224.0/Image_Width)*x_max)\n",
    "            y_max = float((224.0/Image_Height)*y_max)\n",
    "\n",
    "            # 변형시킨걸 x,y,w,h로 만들기 \n",
    "            x = (x_min + x_max)/2.0\n",
    "            y = (y_min + y_max)/2.0\n",
    "            w = x_max - x_min\n",
    "            h = y_max - y_min\n",
    "\n",
    "            # x,y가 속한 cell알아내기\n",
    "            x_cell = int(x/32) # 0~6\n",
    "            y_cell = int(y/32) # 0~6\n",
    "            # cell의 중심 좌표는 (0.5, 0.5)다\n",
    "            x_val_inCell = float((x - x_cell * 32.0)/32.0) # 0.0 ~ 1.0\n",
    "            y_val_inCell = float((y - y_cell * 32.0)/32.0) # 0.0 ~ 1.0\n",
    "\n",
    "            # w, h 를 0~1 사이의 값으로 만들기\n",
    "            w = w / 224.0\n",
    "            h = h / 224.0\n",
    "\n",
    "            class_index_inCell = class_index + 5\n",
    "\n",
    "            label[y_cell][x_cell][0] = x_val_inCell\n",
    "            label[y_cell][x_cell][1] = y_val_inCell\n",
    "            label[y_cell][x_cell][2] = w\n",
    "            label[y_cell][x_cell][3] = h\n",
    "            label[y_cell][x_cell][4] = 1.0\n",
    "            label[y_cell][x_cell][class_index_inCell] = 1.0\n",
    "\n",
    "\n",
    "    # single-object in image\n",
    "    except TypeError as e : \n",
    "        # class의 index 휙득\n",
    "        class_index = Classes_inDataSet.index(xml_file['annotation']['object']['name'].lower())\n",
    "            \n",
    "        # min, max좌표 얻기\n",
    "        x_min = float(xml_file['annotation']['object']['bndbox']['xmin']) \n",
    "        y_min = float(xml_file['annotation']['object']['bndbox']['ymin'])\n",
    "        x_max = float(xml_file['annotation']['object']['bndbox']['xmax']) \n",
    "        y_max = float(xml_file['annotation']['object']['bndbox']['ymax'])\n",
    "\n",
    "        # 224*224에 맞게 변형시켜줌\n",
    "        x_min = float((224.0/Image_Width)*x_min)\n",
    "        y_min = float((224.0/Image_Height)*y_min)\n",
    "        x_max = float((224.0/Image_Width)*x_max)\n",
    "        y_max = float((224.0/Image_Height)*y_max)\n",
    "\n",
    "        # 변형시킨걸 x,y,w,h로 만들기 \n",
    "        x = (x_min + x_max)/2.0\n",
    "        y = (y_min + y_max)/2.0\n",
    "        w = x_max - x_min\n",
    "        h = y_max - y_min\n",
    "\n",
    "        # x,y가 속한 cell알아내기\n",
    "        x_cell = int(x/32) # 0~6\n",
    "        y_cell = int(y/32) # 0~6\n",
    "        x_val_inCell = float((x - x_cell * 32.0)/32.0) # 0.0 ~ 1.0\n",
    "        y_val_inCell = float((y - y_cell * 32.0)/32.0) # 0.0 ~ 1.0\n",
    "\n",
    "        # w, h 를 0~1 사이의 값으로 만들기\n",
    "        w = w / 224.0\n",
    "        h = h / 224.0\n",
    "\n",
    "        class_index_inCell = class_index + 5\n",
    "\n",
    "        label[y_cell][x_cell][0] = x_val_inCell\n",
    "        label[y_cell][x_cell][1] = y_val_inCell\n",
    "        label[y_cell][x_cell][2] = w\n",
    "        label[y_cell][x_cell][3] = h\n",
    "        label[y_cell][x_cell][4] = 1.0\n",
    "        label[y_cell][x_cell][class_index_inCell] = 1.0\n",
    "\n",
    "    return label # np array로 반환\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 데이터 증강을 할거면 여기서 해야한다.\n",
    "def make_dataset(image_file_path_list, xml_file_path_list, Classes_inDataSet) :\n",
    "\n",
    "    image_dataset = []\n",
    "    label_dataset = []\n",
    "\n",
    "    for i in tqdm(range(0, len(image_file_path_list)), desc = \"make dataset\"):\n",
    "        # 이미지를 넘파이 배열로 불러온 뒤 255로 나눠 픽셀별 R, G, B를 0~1사이의 값으로 만들어버린다.\n",
    "        image = cv2.imread(image_file_path_list[i]) \n",
    "        image = cv2.resize(image, (224, 224))/ 255.0 \n",
    "        \n",
    "        label = get_label_fromImage(xml_file_path_list[i], Classes_inDataSet)\n",
    "        \n",
    "        # 이 부분에 데이터 증강을 구현할 수 있으나 구현 난이도 등을 고려해 구현하지 않음\n",
    "        # 테스트 결과, 데이터 증강을 하지 않아 성능이 기대한 것만큼 나오지 않았음 \n",
    "        \n",
    "        image_dataset.append(image)\n",
    "        label_dataset.append(label)\n",
    "    \n",
    "    image_dataset = np.array(image_dataset, dtype=\"object\")\n",
    "    label_dataset = np.array(label_dataset, dtype=\"object\")\n",
    "    \n",
    "    image_dataset = np.reshape(image_dataset, (-1, 224, 224, 3)).astype(np.float32)\n",
    "    label_dataset = np.reshape(label_dataset, (-1, 7, 7, 25))\n",
    "\n",
    "    return image_dataset, tf.convert_to_tensor(label_dataset, dtype=tf.float32)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "Classes_inDataSet = get_Classes_inImage(xml_file_path_list)\n",
    "\n",
    "train_image_dataset, train_label_dataset = make_dataset(image_file_path_list, xml_file_path_list, Classes_inDataSet)\n",
    "val_image_dataset, val_label_dataset = make_dataset(test_image_file_path_list[:1024], test_xml_file_path_list[:1024], Classes_inDataSet)\n",
    "test_image_dataset, test_label_dataset = make_dataset(test_image_file_path_list[1024:], test_xml_file_path_list[1024:], Classes_inDataSet)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "make dataset: 100%|██████████| 5011/5011 [00:25<00:00, 193.36it/s]\n",
      "make dataset: 100%|██████████| 1024/1024 [00:04<00:00, 224.10it/s]\n",
      "make dataset: 100%|██████████| 3928/3928 [00:17<00:00, 223.32it/s]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "max_num = len(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers) # 레이어 최대 개수\n",
    "\n",
    "YOLO = tf.keras.models.Sequential(name = \"YOLO\")\n",
    "for i in range(0, max_num-1):\n",
    "    YOLO.add(tf.keras.applications.VGG16(weights='imagenet', include_top=False,  input_shape=(224, 224, 3)).layers[i])\n",
    "\n",
    "initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
    "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.01)  \n",
    "regularizer = tf.keras.regularizers.l2(0.0005) # L2 규제 == weight decay.\n",
    "\n",
    "# 훈련된 모델은 더이상 건드리지 않기. 논문에서도 1주일 훈련시켰다고 말한 이후로 따로 언급이 없음\n",
    "for layer in YOLO.layers:\n",
    "    # 훈련 X\n",
    "    layer.trainable=False\n",
    "    if (hasattr(layer,'activation'))==True:\n",
    "        layer.activation = leaky_relu\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 원문은 DarkNet을 썼지만 나는 구현을 쉽게 하기 위해 VGG16을 썼다.\n",
    "### 여기에 따로 레이어를 얹어서 YOLO를 구현할거다"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "YOLO.add(tf.keras.layers.Conv2D(1024, (3, 3), activation=leaky_relu, kernel_initializer=initializer, kernel_regularizer = regularizer, padding = 'SAME', name = \"detection_conv1\", dtype='float32'))\n",
    "YOLO.add(tf.keras.layers.Conv2D(1024, (3, 3), activation=leaky_relu, kernel_initializer=initializer, kernel_regularizer = regularizer, padding = 'SAME', name = \"detection_conv2\", dtype='float32'))\n",
    "YOLO.add(tf.keras.layers.MaxPool2D((2, 2)))\n",
    "YOLO.add(tf.keras.layers.Conv2D(1024, (3, 3), activation=leaky_relu, kernel_initializer=initializer, kernel_regularizer = regularizer, padding = 'SAME', name = \"detection_conv3\", dtype='float32'))\n",
    "YOLO.add(tf.keras.layers.Conv2D(1024, (3, 3), activation=leaky_relu, kernel_initializer=initializer, kernel_regularizer = regularizer, padding = 'SAME', name = \"detection_conv4\", dtype='float32'))\n",
    "# Linear 부분\n",
    "YOLO.add(tf.keras.layers.Flatten())\n",
    "YOLO.add(tf.keras.layers.Dense(4096, activation=leaky_relu, kernel_initializer = initializer, kernel_regularizer = regularizer, name = \"detection_linear1\", dtype='float32'))\n",
    "YOLO.add(tf.keras.layers.Dropout(.5))\n",
    "# 마지막 레이어의 활성화 함수는 선형 활성화 함수인데 이건 입력값을 그대로 내보내는거라 activation을 따로 지정하지 않았다.\n",
    "YOLO.add(tf.keras.layers.Dense(1470, kernel_initializer = initializer, kernel_regularizer = regularizer, name = \"detection_linear2\", dtype='float32')) # 7*7*30 = 1470. 0~29 : (0, 0) 위치의 픽셀에 대한 각종 출력값, 30~59 : (1, 0) 위치의...블라블라\n",
    "YOLO.add(tf.keras.layers.Reshape((7, 7, 30), name = 'output', dtype='float32'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def yolo_multitask_loss(y_true, y_pred): # 커스텀 손실함수. 배치 단위로 값이 들어온다\n",
    "    \n",
    "    # YOLOv1의 Loss function은 3개로 나뉜다. localization, confidence, classification\n",
    "    # localization은 추측한 box랑 ground truth box의 오차\n",
    "    \n",
    "    batch_loss = 0\n",
    "    count = len(y_true)\n",
    "    for i in range(0, len(y_true)) :\n",
    "        y_true_unit = tf.identity(y_true[i])\n",
    "        y_pred_unit = tf.identity(y_pred[i])\n",
    "        \n",
    "        y_true_unit = tf.reshape(y_true_unit, [49, 25])\n",
    "        y_pred_unit = tf.reshape(y_pred_unit, [49, 30])\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for j in range(0, len(y_true_unit)) :\n",
    "            # pred = [1, 30], true = [1, 25]\n",
    "            \n",
    "            bbox1_pred = tf.identity(y_pred_unit[j][:4])\n",
    "            bbox1_pred_confidence = tf.identity(y_pred_unit[j][4])\n",
    "            bbox2_pred = tf.identity(y_pred_unit[j][5:9])\n",
    "            bbox2_pred_confidence = tf.identity(y_pred_unit[j][9])\n",
    "            class_pred = tf.identity(y_pred_unit[j][10:])\n",
    "            \n",
    "            bbox_true = tf.identity(y_true_unit[j][:4])\n",
    "            bbox_true_confidence = tf.identity(y_true_unit[j][4])\n",
    "            class_true = tf.identity(y_true_unit[j][5:])\n",
    "            \n",
    "            # IoU 구하기\n",
    "            # x,y,w,h -> min_x, min_y, max_x, max_y로 변환\n",
    "            box_pred_1_np = bbox1_pred.numpy()\n",
    "            box_pred_2_np = bbox2_pred.numpy()\n",
    "            box_true_np   = bbox_true.numpy()\n",
    "\n",
    "            box_pred_1_area = box_pred_1_np[2] * box_pred_1_np[3]\n",
    "            box_pred_2_area = box_pred_2_np[2] * box_pred_2_np[3]\n",
    "            box_true_area   = box_true_np[2]  * box_true_np[3]\n",
    "\n",
    "            box_pred_1_minmax = np.asarray([box_pred_1_np[0] - 0.5*box_pred_1_np[2], box_pred_1_np[1] - 0.5*box_pred_1_np[3], box_pred_1_np[0] + 0.5*box_pred_1_np[2], box_pred_1_np[1] + 0.5*box_pred_1_np[3]])\n",
    "            box_pred_2_minmax = np.asarray([box_pred_2_np[0] - 0.5*box_pred_2_np[2], box_pred_2_np[1] - 0.5*box_pred_2_np[3], box_pred_2_np[0] + 0.5*box_pred_2_np[2], box_pred_2_np[1] + 0.5*box_pred_2_np[3]])\n",
    "            box_true_minmax   = np.asarray([box_true_np[0] - 0.5*box_true_np[2], box_true_np[1] - 0.5*box_true_np[3], box_true_np[0] + 0.5*box_true_np[2], box_true_np[1] + 0.5*box_true_np[3]])\n",
    "\n",
    "            # 곂치는 영역의 (min_x, min_y, max_x, max_y)\n",
    "            InterSection_pred_1_with_true = [max(box_pred_1_minmax[0], box_true_minmax[0]), max(box_pred_1_minmax[1], box_true_minmax[1]), min(box_pred_1_minmax[2], box_true_minmax[2]), min(box_pred_1_minmax[3], box_true_minmax[3])]\n",
    "            InterSection_pred_2_with_true = [max(box_pred_2_minmax[0], box_true_minmax[0]), max(box_pred_2_minmax[1], box_true_minmax[1]), min(box_pred_2_minmax[2], box_true_minmax[2]), min(box_pred_2_minmax[3], box_true_minmax[3])]\n",
    "\n",
    "            # 박스별로 IoU를 구한다\n",
    "            IntersectionArea_pred_1_true = 0\n",
    "\n",
    "            # 음수 * 음수 = 양수일 수도 있으니 검사를 한다.\n",
    "            if (InterSection_pred_1_with_true[2] - InterSection_pred_1_with_true[0] + 1) >= 0 and (InterSection_pred_1_with_true[3] - InterSection_pred_1_with_true[1] + 1) >= 0 :\n",
    "                    IntersectionArea_pred_1_true = (InterSection_pred_1_with_true[2] - InterSection_pred_1_with_true[0] + 1) * InterSection_pred_1_with_true[3] - InterSection_pred_1_with_true[1] + 1\n",
    "\n",
    "            IntersectionArea_pred_2_true = 0\n",
    "\n",
    "            if (InterSection_pred_2_with_true[2] - InterSection_pred_2_with_true[0] + 1) >= 0 and (InterSection_pred_2_with_true[3] - InterSection_pred_2_with_true[1] + 1) >= 0 :\n",
    "                    IntersectionArea_pred_2_true = (InterSection_pred_2_with_true[2] - InterSection_pred_2_with_true[0] + 1) * InterSection_pred_2_with_true[3] - InterSection_pred_2_with_true[1] + 1\n",
    "\n",
    "            Union_pred_1_true = box_pred_1_area + box_true_area - IntersectionArea_pred_1_true\n",
    "            Union_pred_2_true = box_pred_2_area + box_true_area - IntersectionArea_pred_2_true\n",
    "\n",
    "            IoU_box_1 = IntersectionArea_pred_1_true/Union_pred_1_true\n",
    "            IoU_box_2 = IntersectionArea_pred_2_true/Union_pred_2_true\n",
    "                        \n",
    "            responsible_IoU = 0\n",
    "            responsible_box = 0\n",
    "            responsible_bbox_confidence = 0\n",
    "            non_responsible_bbox_confidence = 0\n",
    "\n",
    "            # box1, box2 중 responsible한걸 선택(IoU 기준)\n",
    "            if IoU_box_1 >= IoU_box_2 :\n",
    "                responsible_IoU = IoU_box_1\n",
    "                responsible_box = tf.identity(bbox1_pred)\n",
    "                responsible_bbox_confidence = tf.identity(bbox1_pred_confidence)\n",
    "                non_responsible_bbox_confidence = tf.identity(bbox2_pred_confidence)\n",
    "                                \n",
    "            else :\n",
    "                responsible_IoU = IoU_box_2\n",
    "                responsible_box = tf.identity(bbox2_pred)\n",
    "                responsible_bbox_confidence = tf.identity(bbox2_pred_confidence)\n",
    "                non_responsible_bbox_confidence = tf.identity(bbox1_pred_confidence)\n",
    "                \n",
    "            # 1obj(i) 정하기(해당 셀에 객체의 중심좌표가 들어있는가?)\n",
    "            obj_exist = tf.ones_like(bbox_true_confidence)\n",
    "            if box_true_np[0] == 0.0 and box_true_np[1] == 0.0 and box_true_np[2] == 0.0 and box_true_np[3] == 0.0 : \n",
    "                obj_exist = tf.zeros_like(bbox_true_confidence) \n",
    "            \n",
    "                        \n",
    "            # 만약 해당 cell에 객체가 없으면 confidence error의 no object 파트만 판단. (label된 값에서 알아서 해결)\n",
    "            # 0~3 : bbox1의 위치 정보, 4 : bbox1의 bbox confidence score, 5~8 : bbox2의 위치 정보, 9 : bbox2의 confidence score, 10~29 : cell에 존재하는 클래스 확률 = pr(class | object) \n",
    "\n",
    "            # localization error 구하기(x,y,w,h). x, y는 해당 grid cell의 중심 좌표와 offset이고 w, h는 전체 이미지에 대해 정규화된 값이다. 즉, 범위가 0~1이다.\n",
    "            localization_err_x = tf.math.pow( tf.math.subtract(bbox_true[0], responsible_box[0]), 2) # (x-x_hat)^2\n",
    "            localization_err_y = tf.math.pow( tf.math.subtract(bbox_true[1], responsible_box[1]), 2) # (y-y_hat)^2\n",
    "\n",
    "            localization_err_w = tf.math.pow( tf.math.subtract(tf.sqrt(bbox_true[2]), tf.sqrt(responsible_box[2])), 2) # (sqrt(w) - sqrt(w_hat))^2\n",
    "            localization_err_h = tf.math.pow( tf.math.subtract(tf.sqrt(bbox_true[3]), tf.sqrt(responsible_box[3])), 2) # (sqrt(h) - sqrt(h_hat))^2\n",
    "            \n",
    "            # nan 방지\n",
    "            if tf.math.is_nan(localization_err_w).numpy() == True :\n",
    "                localization_err_w = tf.zeros_like(localization_err_w, dtype=tf.float32)\n",
    "            \n",
    "            if tf.math.is_nan(localization_err_h).numpy() == True :\n",
    "                localization_err_h = tf.zeros_like(localization_err_h, dtype=tf.float32)\n",
    "            \n",
    "            localization_err_1 = tf.math.add(localization_err_x, localization_err_y)\n",
    "            localization_err_2 = tf.math.add(localization_err_w, localization_err_h)\n",
    "            localization_err = tf.math.add(localization_err_1, localization_err_2)\n",
    "            \n",
    "            weighted_localization_err = tf.math.multiply(localization_err, 5.0) # 5.0 : λ_coord\n",
    "            weighted_localization_err = tf.math.multiply(weighted_localization_err, obj_exist) # 1obj(i) 곱하기\n",
    "            \n",
    "            # confidence error 구하기. true의 경우 답인 객체는 1 * ()고 아니면 0*()가 된다. \n",
    "            # index 4, 9에 있는 값(0~1)이 해당 박스에 객체가 있을 확률을 나타낸거다. Pr(obj in bbox)\n",
    "            \n",
    "            class_confidence_score_obj = tf.math.pow(tf.math.subtract(responsible_bbox_confidence, bbox_true_confidence), 2)\n",
    "            class_confidence_score_noobj = tf.math.pow(tf.math.subtract(non_responsible_bbox_confidence, tf.zeros_like(bbox_true_confidence)), 2)\n",
    "            class_confidence_score_noobj = tf.math.multiply(class_confidence_score_noobj, 0.5)\n",
    "            \n",
    "            class_confidence_score_obj = tf.math.multiply(class_confidence_score_obj, obj_exist)\n",
    "            class_confidence_score_noobj = tf.math.multiply(class_confidence_score_noobj, tf.math.subtract(tf.ones_like(obj_exist), obj_exist)) # 객체가 존재하면 0, 존재하지 않으면 1을 곱합\n",
    "            \n",
    "            class_confidence_score = tf.math.add(class_confidence_score_obj,  class_confidence_score_noobj) \n",
    "            \n",
    "            # classification loss(10~29. 인덱스 10~29에 해당되는 값은 Pr(Class_i|Object)이다. 객체가 cell안에 있을 때 해당 객체일 확률\n",
    "            # class_true_oneCell는 진짜 객체의 인덱스에 해당하ㄴ 원소의 값만 1이고 나머지는 0 \n",
    "            \n",
    "            tf.math.pow(tf.math.subtract(class_true, class_pred), 2.0) # 여기서 에러\n",
    "            \n",
    "            classification_err = tf.math.pow(tf.math.subtract(class_true, class_pred), 2.0)\n",
    "            classification_err = tf.math.reduce_sum(classification_err)\n",
    "            classification_err = tf.math.multiply(classification_err, obj_exist)\n",
    "            \n",
    "            # loss합체\n",
    "            loss_OneCell_1 = tf.math.add(weighted_localization_err, class_confidence_score)\n",
    "            \n",
    "            loss_OneCell = tf.math.add(loss_OneCell_1, classification_err)\n",
    "            \n",
    "            if loss == 0 :\n",
    "                loss = tf.identity(loss_OneCell)\n",
    "            else :\n",
    "                loss = tf.math.add(loss, loss_OneCell)\n",
    "        \n",
    "        if batch_loss == 0 :\n",
    "            batch_loss = tf.identity(loss)\n",
    "        else :\n",
    "            batch_loss = tf.math.add(batch_loss, loss)\n",
    "        \n",
    "    # 배치에 대한 loss 구하기\n",
    "    count = tf.Variable(float(count))\n",
    "    batch_loss = tf.math.divide(batch_loss, count)\n",
    "    \n",
    "    return batch_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 가중치 로드\n",
    "# yolo-minkyuKim.h5는 val_loss = 13.44041\n",
    "# YOLO.load_weights('yolo-minkyuKim_2.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCH = 135\n",
    "\n",
    "# \"We continue training with 10−2 for 75 epochs, then 10−3 for 30 epochs, and finally 10−4 for 30 epochs\" 구현\n",
    "def lr_schedule(epoch, lr): # epoch는 0부터 시작\n",
    "    if epoch >=0 and epoch < 75 :\n",
    "        lr = 0.001 + 0.009 * (float(epoch)/(75.0)) # 가중치를 0.001 ~ 0.01로 변경\n",
    "        return lr\n",
    "    elif epoch >= 75 and epoch < 105 :\n",
    "        lr = 0.001\n",
    "        return lr\n",
    "    else : \n",
    "        lr = 0.0001\n",
    "        return lr\n",
    "\n",
    "# loss 제일 낮을 때 가중치 저장\n",
    "filename = 'yolo-minkyuKim.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filename,             # file명을 지정합니다\n",
    "                             verbose=1,            # 로그를 출력합니다\n",
    "                             save_best_only=True   # 가장 best 값만 저장합니다\n",
    "                            )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0.9)\n",
    "\n",
    "YOLO.compile(loss = yolo_multitask_loss, optimizer=optimizer, run_eagerly=True)\n",
    "\n",
    "YOLO.fit(train_image_dataset, train_label_dataset,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          validation_data = (val_image_dataset, val_label_dataset),\n",
    "          epochs=EPOCH,\n",
    "          verbose=1,\n",
    "          callbacks=[checkpoint, tf.keras.callbacks.LearningRateScheduler(lr_schedule)])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 5011 samples, validate on 1024 samples\n",
      "Epoch 1/16\n",
      "4992/5011 [============================>.] - ETA: 13s - loss: 8.7867\n",
      "Epoch 00001: val_loss improved from inf to 13.45902, saving model to yolo-minkyuKim.h5\n",
      "5011/5011 [==============================] - 3777s 754ms/sample - loss: 8.7856 - val_loss: 13.4590\n",
      "Epoch 2/16\n",
      "4992/5011 [============================>.] - ETA: 13s - loss: 8.7869\n",
      "Epoch 00002: val_loss improved from 13.45902 to 13.45582, saving model to yolo-minkyuKim.h5\n",
      "5011/5011 [==============================] - 3759s 750ms/sample - loss: 8.7877 - val_loss: 13.4558\n",
      "Epoch 3/16\n",
      "4992/5011 [============================>.] - ETA: 13s - loss: 8.7907\n",
      "Epoch 00003: val_loss improved from 13.45582 to 13.45425, saving model to yolo-minkyuKim.h5\n",
      "5011/5011 [==============================] - 3761s 751ms/sample - loss: 8.7912 - val_loss: 13.4543\n",
      "Epoch 4/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7762\n",
      "Epoch 00004: val_loss did not improve from 13.45425\n",
      "5011/5011 [==============================] - 3629s 724ms/sample - loss: 8.7770 - val_loss: 13.4545\n",
      "Epoch 5/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7785\n",
      "Epoch 00005: val_loss did not improve from 13.45425\n",
      "5011/5011 [==============================] - 3521s 703ms/sample - loss: 8.7790 - val_loss: 13.4571\n",
      "Epoch 6/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7884\n",
      "Epoch 00006: val_loss did not improve from 13.45425\n",
      "5011/5011 [==============================] - 3521s 703ms/sample - loss: 8.7878 - val_loss: 13.4558\n",
      "Epoch 7/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7763\n",
      "Epoch 00007: val_loss improved from 13.45425 to 13.45278, saving model to yolo-minkyuKim.h5\n",
      "5011/5011 [==============================] - 3540s 706ms/sample - loss: 8.7780 - val_loss: 13.4528\n",
      "Epoch 8/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7715\n",
      "Epoch 00008: val_loss did not improve from 13.45278\n",
      "5011/5011 [==============================] - 3518s 702ms/sample - loss: 8.7697 - val_loss: 13.4585\n",
      "Epoch 9/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7688\n",
      "Epoch 00009: val_loss improved from 13.45278 to 13.44776, saving model to yolo-minkyuKim.h5\n",
      "5011/5011 [==============================] - 3493s 697ms/sample - loss: 8.7706 - val_loss: 13.4478\n",
      "Epoch 10/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7497\n",
      "Epoch 00010: val_loss did not improve from 13.44776\n",
      "5011/5011 [==============================] - 3485s 696ms/sample - loss: 8.7495 - val_loss: 13.4548\n",
      "Epoch 11/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7601\n",
      "Epoch 00011: val_loss did not improve from 13.44776\n",
      "5011/5011 [==============================] - 3484s 695ms/sample - loss: 8.7584 - val_loss: 13.4521\n",
      "Epoch 12/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7552\n",
      "Epoch 00012: val_loss did not improve from 13.44776\n",
      "5011/5011 [==============================] - 3501s 699ms/sample - loss: 8.7554 - val_loss: 13.4492\n",
      "Epoch 13/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7588\n",
      "Epoch 00013: val_loss improved from 13.44776 to 13.44532, saving model to yolo-minkyuKim.h5\n",
      "5011/5011 [==============================] - 3566s 712ms/sample - loss: 8.7591 - val_loss: 13.4453\n",
      "Epoch 14/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7484\n",
      "Epoch 00014: val_loss improved from 13.44532 to 13.44452, saving model to yolo-minkyuKim.h5\n",
      "5011/5011 [==============================] - 3674s 733ms/sample - loss: 8.7505 - val_loss: 13.4445\n",
      "Epoch 15/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7563\n",
      "Epoch 00015: val_loss improved from 13.44452 to 13.44041, saving model to yolo-minkyuKim.h5\n",
      "5011/5011 [==============================] - 3642s 727ms/sample - loss: 8.7567 - val_loss: 13.4404\n",
      "Epoch 16/16\n",
      "4992/5011 [============================>.] - ETA: 12s - loss: 8.7575\n",
      "Epoch 00016: val_loss did not improve from 13.44041\n",
      "5011/5011 [==============================] - 3650s 728ms/sample - loss: 8.7580 - val_loss: 13.4447\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efca4101470>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {
    "scrolled": false
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5de1033c4b6bab6c838dd6f2b29357493acb89b798a708ce20db74bb033faa6b"
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}